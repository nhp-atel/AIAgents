{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building AI Agents with LangGraph\n",
    "\n",
    "## What is LangGraph?\n",
    "\n",
    "**LangGraph** is a framework for building stateful, multi-step AI agent applications as **graphs**. It is part of the LangChain ecosystem but designed specifically for agent orchestration where you need fine-grained control over execution flow.\n",
    "\n",
    "Unlike simple chain-based approaches, LangGraph lets you define **cyclic graphs** — meaning your agents can loop, retry, and route dynamically based on LLM decisions.\n",
    "\n",
    "### Key Concepts at a Glance\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **State** | A shared data structure (typically a `TypedDict`) that flows through the graph. Every node reads from and writes to this state. |\n",
    "| **Nodes** | Python functions that perform work — call an LLM, run a tool, transform data. Each node receives the current state and returns a partial update. |\n",
    "| **Edges** | Connections between nodes. Can be *normal* (always follow) or *conditional* (route based on state). |\n",
    "| **Tools** | Functions the LLM can call. Defined with the `@tool` decorator and bound to the model via `bind_tools()`. |\n",
    "| **Compile & Run** | Once the graph is defined, you `.compile()` it into a runnable. Then `.invoke()` or `.stream()` to execute. |\n",
    "\n",
    "### What you'll build in this notebook\n",
    "\n",
    "1. Understand each core component with standalone examples\n",
    "2. Build a **ReAct agent** (single agent + tools)\n",
    "3. Add **memory & persistence** for multi-turn conversations\n",
    "4. Build a **multi-agent research team** with a supervisor pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install -q langgraph langchain langchain-openai langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Set your OpenAI API key.\n",
    "# The notebook reads from the environment first; if not set, it prompts you.\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "print(\"API key is configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Core Components Deep Dive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 — State\n",
    "\n",
    "The **state** is the central data structure that every node in your graph reads from and writes to. You define it as a `TypedDict`.\n",
    "\n",
    "A critical concept is **reducers**. When a node returns a partial state update, the reducer determines *how* that update is merged into the existing state.\n",
    "\n",
    "- **Default behavior**: the new value **overwrites** the old value.\n",
    "- **`add_messages` reducer**: instead of overwriting, it **appends** new messages to the existing list. This is essential for chat-based agents where you want to accumulate a conversation history.\n",
    "\n",
    "You apply a reducer using `typing.Annotated`:\n",
    "\n",
    "```python\n",
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]  # append, don't overwrite\n",
    "    counter: int                              # plain overwrite\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# Define a simple state with the add_messages reducer\n",
    "class DemoState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    counter: int\n",
    "\n",
    "# Simulate what the reducer does:\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "existing = [HumanMessage(content=\"Hello\")]\n",
    "new = [AIMessage(content=\"Hi there!\")]\n",
    "\n",
    "# add_messages appends instead of replacing\n",
    "merged = add_messages(existing, new)\n",
    "for m in merged:\n",
    "    print(f\"{m.type}: {m.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 — Nodes\n",
    "\n",
    "A **node** is simply a Python function that:\n",
    "1. Receives the current `state` dict as its argument.\n",
    "2. Does some work (call an LLM, run a tool, transform data).\n",
    "3. Returns a **partial state update** — a dict containing only the keys you want to update.\n",
    "\n",
    "```python\n",
    "def my_node(state: AgentState) -> dict:\n",
    "    # read from state\n",
    "    messages = state[\"messages\"]\n",
    "    # do work ...\n",
    "    return {\"messages\": [new_message]}  # partial update\n",
    "```\n",
    "\n",
    "The returned dict is merged into the state using the reducers defined in your `TypedDict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: a node that increments a counter and adds a message\n",
    "def greeter_node(state: DemoState) -> dict:\n",
    "    \"\"\"A simple node that greets and increments a counter.\"\"\"\n",
    "    count = state.get(\"counter\", 0) + 1\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"Greeting #{count}!\")],\n",
    "        \"counter\": count,\n",
    "    }\n",
    "\n",
    "# We'll wire this into a graph shortly. For now, let's test the logic:\n",
    "result = greeter_node({\"messages\": [], \"counter\": 0})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 — Edges\n",
    "\n",
    "Edges define how execution flows between nodes.\n",
    "\n",
    "| Edge type | Method | Description |\n",
    "|-----------|--------|-------------|\n",
    "| Normal | `graph.add_edge(\"a\", \"b\")` | Always go from node A to node B |\n",
    "| Conditional | `graph.add_conditional_edges(\"a\", routing_fn, mapping)` | Call `routing_fn(state)` to decide which node to go to next |\n",
    "| Entry | `graph.add_edge(START, \"a\")` | Where the graph begins |\n",
    "| Terminal | `graph.add_edge(\"a\", END)` | Where the graph ends |\n",
    "\n",
    "`START` and `END` are special sentinel nodes imported from `langgraph.graph`.\n",
    "\n",
    "Conditional edges are the backbone of agent loops — they let the LLM decide whether to call a tool, route to another agent, or finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Build a tiny graph: START -> greeter -> END\n",
    "tiny_graph = StateGraph(DemoState)\n",
    "tiny_graph.add_node(\"greeter\", greeter_node)\n",
    "tiny_graph.add_edge(START, \"greeter\")\n",
    "tiny_graph.add_edge(\"greeter\", END)\n",
    "\n",
    "app = tiny_graph.compile()\n",
    "\n",
    "# Run it\n",
    "result = app.invoke({\"messages\": [HumanMessage(content=\"Hi!\")], \"counter\": 0})\n",
    "print(f\"Counter: {result['counter']}\")\n",
    "for m in result[\"messages\"]:\n",
    "    print(f\"  {m.type}: {m.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 — Tools\n",
    "\n",
    "Tools let the LLM take **actions** — search the web, look up data, perform calculations, etc.\n",
    "\n",
    "In LangChain/LangGraph, you:\n",
    "1. Define tools using the `@tool` decorator from `langchain_core.tools`.\n",
    "2. Bind them to a chat model with `model.bind_tools([tool1, tool2])`.\n",
    "3. The LLM will include `tool_calls` in its response when it wants to use a tool.\n",
    "4. A **tool node** executes those calls and returns the results as `ToolMessage`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define a mock tool\n",
    "@tool\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers together.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "# Create a model and bind the tool\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools([add_numbers])\n",
    "\n",
    "# Ask the model something that should trigger the tool\n",
    "response = llm_with_tools.invoke(\"What is 42 + 58?\")\n",
    "\n",
    "# The model doesn't compute the answer directly — it emits a tool_call\n",
    "print(\"Tool calls:\", response.tool_calls)\n",
    "print(\"Content:\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Build a Simple ReAct Agent\n",
    "\n",
    "The **ReAct** (Reason + Act) pattern is the most common agent architecture:\n",
    "\n",
    "1. The LLM **reasons** about what to do.\n",
    "2. It decides to **act** by calling a tool (or to finish).\n",
    "3. The tool result is fed back, and the loop repeats.\n",
    "\n",
    "```\n",
    "START -> agent -> (tool_calls?) --yes--> tools -> agent (loop)\n",
    "                                --no---> END\n",
    "```\n",
    "\n",
    "Let's build this from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "# --- 1. Define mock tools ---\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get the current weather for a city.\"\"\"\n",
    "    # Mock data — no external API needed\n",
    "    weather_data = {\n",
    "        \"new york\": \"72°F, partly cloudy, humidity 55%\",\n",
    "        \"london\": \"58°F, overcast with light rain, humidity 80%\",\n",
    "        \"tokyo\": \"68°F, clear skies, humidity 45%\",\n",
    "        \"paris\": \"63°F, sunny with occasional clouds, humidity 50%\",\n",
    "        \"sydney\": \"81°F, sunny, humidity 40%\",\n",
    "    }\n",
    "    return weather_data.get(\n",
    "        city.lower(),\n",
    "        f\"Weather data not available for {city}. Try: New York, London, Tokyo, Paris, Sydney.\"\n",
    "    )\n",
    "\n",
    "\n",
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search for information on a topic. Returns a brief summary.\"\"\"\n",
    "    # Mock search results\n",
    "    results = {\n",
    "        \"langgraph\": \"LangGraph is a library by LangChain for building stateful, multi-actor AI applications with LLMs as graphs.\",\n",
    "        \"python\": \"Python is a high-level, general-purpose programming language created by Guido van Rossum, released in 1991.\",\n",
    "        \"react agent\": \"ReAct (Reason + Act) agents interleave reasoning and action steps, letting LLMs decide when to call tools.\",\n",
    "    }\n",
    "    query_lower = query.lower()\n",
    "    for key, value in results.items():\n",
    "        if key in query_lower:\n",
    "            return value\n",
    "    return f\"Search results for '{query}': This is a mock search. In production, connect to a real search API.\"\n",
    "\n",
    "\n",
    "tools = [get_weather, search]\n",
    "print(f\"Defined {len(tools)} tools: {[t.name for t in tools]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Create the model with tools bound ---\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "\n",
    "\n",
    "# --- 3. Define the state ---\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "# --- 4. Define the agent node ---\n",
    "\n",
    "def agent_node(state: AgentState) -> dict:\n",
    "    \"\"\"Call the LLM with the current messages. It will decide whether to use tools.\"\"\"\n",
    "    response = model_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# --- 5. Define the routing function ---\n",
    "\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    \"\"\"Check if the last message has tool calls. If yes, route to tools; otherwise, end.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "\n",
    "# --- 6. Build the graph ---\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "graph.add_node(\"agent\", agent_node)\n",
    "graph.add_node(\"tools\", ToolNode(tools))  # ToolNode auto-executes tool calls\n",
    "\n",
    "# Add edges\n",
    "graph.add_edge(START, \"agent\")                          # Entry point\n",
    "graph.add_conditional_edges(\"agent\", should_continue)   # Agent decides: tools or END\n",
    "graph.add_edge(\"tools\", \"agent\")                        # After tools, go back to agent\n",
    "\n",
    "# Compile\n",
    "react_agent = graph.compile()\n",
    "\n",
    "print(\"ReAct agent compiled successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Visualize the graph ---\n",
    "\n",
    "print(react_agent.get_graph().draw_mermaid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. Run the agent! ---\n",
    "\n",
    "result = react_agent.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"What's the weather like in Tokyo and Paris?\")]\n",
    "})\n",
    "\n",
    "# Print the full conversation\n",
    "for msg in result[\"messages\"]:\n",
    "    role = msg.type\n",
    "    if role == \"human\":\n",
    "        print(f\"\\nUser: {msg.content}\")\n",
    "    elif role == \"ai\":\n",
    "        if msg.tool_calls:\n",
    "            for tc in msg.tool_calls:\n",
    "                print(f\"\\nAgent -> Tool Call: {tc['name']}({tc['args']})\")\n",
    "        if msg.content:\n",
    "            print(f\"\\nAgent: {msg.content}\")\n",
    "    elif role == \"tool\":\n",
    "        print(f\"  Tool ({msg.name}): {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a search query too\n",
    "result2 = react_agent.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"Search for information about LangGraph.\")]\n",
    "})\n",
    "\n",
    "for msg in result2[\"messages\"]:\n",
    "    if msg.type == \"ai\" and msg.content:\n",
    "        print(f\"Agent: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Adding Memory & Persistence\n",
    "\n",
    "By default, each `.invoke()` call is stateless — the agent has no memory of previous conversations.\n",
    "\n",
    "LangGraph solves this with **checkpointers**. A checkpointer saves the graph state after each step. When you invoke the graph with the same `thread_id`, it loads the previous state and continues from where it left off.\n",
    "\n",
    "**Short-term memory** (within a thread): The agent remembers everything said in the current conversation thread.\n",
    "\n",
    "**Long-term memory** (across threads): Requires external storage (database, vector store). Not built into the basic checkpointer, but LangGraph supports custom checkpointer backends for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Create a checkpointer (in-memory for this demo)\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Recompile the same graph with the checkpointer attached\n",
    "react_agent_with_memory = graph.compile(checkpointer=memory)\n",
    "\n",
    "print(\"Agent recompiled with memory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation turn 1 — use a thread_id to track this conversation\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "result1 = react_agent_with_memory.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is the weather in London?\")]},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Show the agent's final answer\n",
    "print(\"Turn 1 answer:\")\n",
    "print(result1[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation turn 2 — same thread_id, so it remembers turn 1\n",
    "result2 = react_agent_with_memory.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"How about New York? Is it warmer than the city I just asked about?\")]},\n",
    "    config=config  # Same thread_id!\n",
    ")\n",
    "\n",
    "print(\"Turn 2 answer:\")\n",
    "print(result2[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: the agent's state has the full conversation history\n",
    "print(f\"Total messages in thread: {len(result2['messages'])}\")\n",
    "print(\"\\n--- Full conversation ---\")\n",
    "for msg in result2[\"messages\"]:\n",
    "    if msg.type == \"human\":\n",
    "        print(f\"\\nUser: {msg.content}\")\n",
    "    elif msg.type == \"ai\" and msg.content:\n",
    "        print(f\"Agent: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different thread = fresh conversation, no memory of previous thread\n",
    "config_new_thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "result_new = react_agent_with_memory.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Which city did I just ask about?\")]},\n",
    "    config=config_new_thread\n",
    ")\n",
    "\n",
    "print(\"New thread answer (no memory of thread 1):\")\n",
    "print(result_new[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Multi-Agent System — Research Team\n",
    "\n",
    "Now let's build something more advanced: a **supervisor-based multi-agent system**.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "START -> Supervisor --\"researcher\"--> Researcher -> Supervisor\n",
    "                   --\"writer\"------> Writer     -> Supervisor\n",
    "                   --\"FINISH\"------> END\n",
    "```\n",
    "\n",
    "- **Supervisor**: An LLM that reads the conversation and decides which worker to route to next (or to finish).\n",
    "- **Researcher**: Has access to a search tool. Gathers information.\n",
    "- **Writer**: Takes the research and produces a polished, well-structured output.\n",
    "\n",
    "This pattern is powerful because each agent can be specialized, and the supervisor provides dynamic orchestration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict, Literal\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "# --- 1. Define the shared state ---\n",
    "\n",
    "class TeamState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    next: str  # Which agent to route to next\n",
    "\n",
    "\n",
    "# --- 2. Define tools for the Researcher ---\n",
    "\n",
    "@tool\n",
    "def research_search(query: str) -> str:\n",
    "    \"\"\"Search for research information on a topic.\"\"\"\n",
    "    # Expanded mock search with richer data\n",
    "    knowledge_base = {\n",
    "        \"artificial intelligence\": (\n",
    "            \"AI is a branch of computer science focused on creating systems that can perform tasks \"\n",
    "            \"requiring human intelligence. Key areas include machine learning, NLP, computer vision, \"\n",
    "            \"and robotics. The global AI market is projected to reach $1.8 trillion by 2030. \"\n",
    "            \"Major players include OpenAI, Google DeepMind, Anthropic, and Meta AI.\"\n",
    "        ),\n",
    "        \"climate change\": (\n",
    "            \"Climate change refers to long-term shifts in global temperatures and weather patterns. \"\n",
    "            \"Human activities, particularly burning fossil fuels, have been the main driver since the 1800s. \"\n",
    "            \"The Paris Agreement aims to limit warming to 1.5°C. Renewable energy adoption is accelerating, \"\n",
    "            \"with solar and wind now cheaper than coal in most regions.\"\n",
    "        ),\n",
    "        \"quantum computing\": (\n",
    "            \"Quantum computing leverages quantum mechanical phenomena like superposition and entanglement \"\n",
    "            \"to process information. IBM, Google, and startups like IonQ are leading development. \"\n",
    "            \"Potential applications include drug discovery, cryptography, and optimization problems.\"\n",
    "        ),\n",
    "    }\n",
    "    query_lower = query.lower()\n",
    "    for key, value in knowledge_base.items():\n",
    "        if key in query_lower:\n",
    "            return value\n",
    "    return (\n",
    "        f\"Research on '{query}': This is a rapidly evolving field with significant recent developments. \"\n",
    "        f\"Key trends include increased automation, global collaboration, and interdisciplinary approaches.\"\n",
    "    )\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_statistics(topic: str) -> str:\n",
    "    \"\"\"Get key statistics and data points about a topic.\"\"\"\n",
    "    stats = {\n",
    "        \"ai\": \"Global AI market: $196B (2023), projected $1.8T by 2030. AI adoption rate: 35% of companies.\",\n",
    "        \"climate\": \"Global temp rise: +1.1°C since pre-industrial. CO2 levels: 421 ppm. Renewable energy: 30% of global electricity.\",\n",
    "        \"quantum\": \"Quantum computing market: $1.3B (2024). Qubits achieved: 1,000+ (IBM). Estimated practical advantage: 2028-2032.\",\n",
    "    }\n",
    "    topic_lower = topic.lower()\n",
    "    for key, value in stats.items():\n",
    "        if key in topic_lower:\n",
    "            return value\n",
    "    return f\"Key statistics for '{topic}' are currently being compiled. Check specialized databases for the latest figures.\"\n",
    "\n",
    "\n",
    "research_tools = [research_search, get_statistics]\n",
    "print(\"Research tools defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Define the Supervisor node ---\n",
    "\n",
    "supervisor_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "SUPERVISOR_SYSTEM_PROMPT = \"\"\"You are a supervisor managing a research team. Your team members are:\n",
    "\n",
    "- **researcher**: Gathers information and data using search tools. Send work here when you need facts, data, or research.\n",
    "- **writer**: Writes polished, well-structured content based on available research. Send work here when enough research has been gathered.\n",
    "\n",
    "Given the conversation so far, decide which team member should act next, or if the task is complete.\n",
    "\n",
    "Respond with ONLY one of: \"researcher\", \"writer\", or \"FINISH\".\n",
    "\n",
    "Guidelines:\n",
    "- Start by sending the task to the researcher to gather information.\n",
    "- Once sufficient research is available, send it to the writer.\n",
    "- After the writer produces output, respond with FINISH.\n",
    "- If the user's request is simple and doesn't need research, go directly to the writer.\"\"\"\n",
    "\n",
    "\n",
    "def supervisor_node(state: TeamState) -> dict:\n",
    "    \"\"\"The supervisor decides which agent to route to next.\"\"\"\n",
    "    messages = [\n",
    "        SystemMessage(content=SUPERVISOR_SYSTEM_PROMPT),\n",
    "        *state[\"messages\"],\n",
    "        HumanMessage(content=\"Who should act next? Respond with ONLY: researcher, writer, or FINISH.\")\n",
    "    ]\n",
    "    response = supervisor_llm.invoke(messages)\n",
    "    decision = response.content.strip().lower().replace('\"', '').replace(\"'\", \"\")\n",
    "\n",
    "    # Normalize the decision\n",
    "    if \"researcher\" in decision:\n",
    "        next_agent = \"researcher\"\n",
    "    elif \"writer\" in decision:\n",
    "        next_agent = \"writer\"\n",
    "    else:\n",
    "        next_agent = \"FINISH\"\n",
    "\n",
    "    return {\n",
    "        \"next\": next_agent,\n",
    "        \"messages\": [AIMessage(content=f\"[Supervisor] Routing to: {next_agent}\")]\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Supervisor node defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Define the Researcher node ---\n",
    "\n",
    "researcher_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0).bind_tools(research_tools)\n",
    "\n",
    "RESEARCHER_SYSTEM_PROMPT = \"\"\"You are a research specialist. Your job is to gather comprehensive information\n",
    "about the topic using your available tools (research_search, get_statistics).\n",
    "\n",
    "Always use your tools to find information. Summarize your findings clearly.\"\"\"\n",
    "\n",
    "\n",
    "def researcher_node(state: TeamState) -> dict:\n",
    "    \"\"\"The researcher gathers information using tools.\"\"\"\n",
    "    messages = [SystemMessage(content=RESEARCHER_SYSTEM_PROMPT), *state[\"messages\"]]\n",
    "    response = researcher_llm.invoke(messages)\n",
    "\n",
    "    # If the LLM wants to call tools, execute them inline for simplicity\n",
    "    if response.tool_calls:\n",
    "        tool_map = {t.name: t for t in research_tools}\n",
    "        tool_results = []\n",
    "        for tc in response.tool_calls:\n",
    "            result = tool_map[tc[\"name\"]].invoke(tc[\"args\"])\n",
    "            tool_results.append(f\"{tc['name']}: {result}\")\n",
    "\n",
    "        summary = \"\\n\".join(tool_results)\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"[Researcher] Findings:\\n{summary}\")]\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"[Researcher] {response.content}\")]\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"Researcher node defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Define the Writer node ---\n",
    "\n",
    "writer_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "WRITER_SYSTEM_PROMPT = \"\"\"You are a professional writer. Based on the research findings in the conversation,\n",
    "produce a polished, well-structured summary. Use clear headings, bullet points, and concise language.\n",
    "Write for a general audience.\"\"\"\n",
    "\n",
    "\n",
    "def writer_node(state: TeamState) -> dict:\n",
    "    \"\"\"The writer produces polished output from research.\"\"\"\n",
    "    messages = [SystemMessage(content=WRITER_SYSTEM_PROMPT), *state[\"messages\"]]\n",
    "    response = writer_llm.invoke(messages)\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"[Writer] {response.content}\")]\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Writer node defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Build the multi-agent graph ---\n",
    "\n",
    "def route_from_supervisor(state: TeamState) -> str:\n",
    "    \"\"\"Route based on the supervisor's decision.\"\"\"\n",
    "    next_agent = state.get(\"next\", \"FINISH\")\n",
    "    if next_agent == \"FINISH\":\n",
    "        return END\n",
    "    return next_agent\n",
    "\n",
    "\n",
    "# Create the graph\n",
    "team_graph = StateGraph(TeamState)\n",
    "\n",
    "# Add nodes\n",
    "team_graph.add_node(\"supervisor\", supervisor_node)\n",
    "team_graph.add_node(\"researcher\", researcher_node)\n",
    "team_graph.add_node(\"writer\", writer_node)\n",
    "\n",
    "# Entry: always start at supervisor\n",
    "team_graph.add_edge(START, \"supervisor\")\n",
    "\n",
    "# Supervisor routes conditionally\n",
    "team_graph.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    route_from_supervisor,\n",
    "    {\"researcher\": \"researcher\", \"writer\": \"writer\", END: END}\n",
    ")\n",
    "\n",
    "# Workers always report back to supervisor\n",
    "team_graph.add_edge(\"researcher\", \"supervisor\")\n",
    "team_graph.add_edge(\"writer\", \"supervisor\")\n",
    "\n",
    "# Compile\n",
    "research_team = team_graph.compile()\n",
    "\n",
    "print(\"Multi-agent research team compiled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the multi-agent graph\n",
    "print(research_team.get_graph().draw_mermaid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Run the research team ---\n",
    "\n",
    "result = research_team.invoke({\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"Write a brief report on the current state of artificial intelligence.\")\n",
    "    ],\n",
    "    \"next\": \"\"\n",
    "})\n",
    "\n",
    "# Print the full conversation flow\n",
    "print(\"=\" * 70)\n",
    "print(\"RESEARCH TEAM — FULL CONVERSATION FLOW\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for msg in result[\"messages\"]:\n",
    "    if msg.type == \"human\":\n",
    "        print(f\"\\n{'─'*50}\")\n",
    "        print(f\"USER: {msg.content}\")\n",
    "    elif msg.type == \"ai\" and msg.content:\n",
    "        # Determine which agent spoke\n",
    "        print(f\"\\n{'─'*50}\")\n",
    "        print(msg.content[:500])  # Truncate very long outputs for readability\n",
    "        if len(msg.content) > 500:\n",
    "            print(\"... [truncated for display]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the writer's final output in full\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL OUTPUT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for msg in reversed(result[\"messages\"]):\n",
    "    if msg.type == \"ai\" and msg.content.startswith(\"[Writer]\"):\n",
    "        print(msg.content.replace(\"[Writer] \", \"\"))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Key Takeaways\n",
    "\n",
    "### When to use LangGraph\n",
    "\n",
    "LangGraph is the right choice when you need:\n",
    "\n",
    "- **Fine-grained control** over agent execution flow (custom routing, branching, loops)\n",
    "- **Complex state management** beyond simple message passing\n",
    "- **Cyclic graphs** — agents that can loop, retry, and self-correct\n",
    "- **Multi-agent orchestration** with custom coordination patterns\n",
    "- **Built-in persistence and memory** via checkpointers\n",
    "- **Streaming** and step-by-step observability into agent behavior\n",
    "\n",
    "### LangGraph vs. higher-level frameworks\n",
    "\n",
    "| | LangGraph | CrewAI / AutoGen |\n",
    "|---|-----------|------------------|\n",
    "| **Abstraction level** | Low-level, graph-based | High-level, role-based |\n",
    "| **Control** | Full control over every edge and node | More opinionated, less customizable |\n",
    "| **Learning curve** | Steeper — you build the graph yourself | Gentler — define roles and goals |\n",
    "| **Flexibility** | Arbitrary graph topologies | Mostly linear/sequential flows |\n",
    "| **State management** | Custom TypedDict with reducers | Built-in but less flexible |\n",
    "| **Best for** | Production systems, complex workflows | Prototyping, simpler multi-agent setups |\n",
    "\n",
    "### What we covered\n",
    "\n",
    "1. **Core concepts**: State, Nodes, Edges, Tools — the building blocks of every LangGraph application.\n",
    "2. **ReAct agent**: The fundamental single-agent + tools loop.\n",
    "3. **Memory**: Using `MemorySaver` and thread IDs for multi-turn conversations.\n",
    "4. **Multi-agent systems**: The supervisor pattern for coordinating specialized agents.\n",
    "\n",
    "### Next steps\n",
    "\n",
    "- Explore **human-in-the-loop** with `interrupt_before` / `interrupt_after` to pause the graph for user approval.\n",
    "- Try **streaming** with `.stream()` and `.astream_events()` for real-time output.\n",
    "- Use **LangGraph Platform** (LangGraph Cloud) to deploy agents as APIs.\n",
    "- Implement **persistent checkpointers** (e.g., SQLite, PostgreSQL) for production memory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}